{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f515085",
   "metadata": {},
   "source": [
    "# Model Testing & Time Span Analysis\n",
    "\n",
    "## Overview:\n",
    "This notebook performs detailed model testing across different time spans:\n",
    "\n",
    "**Models (4):**\n",
    "1. Linear Regression\n",
    "2. LSTM/BiLSTM\n",
    "3. Transformer\n",
    "4. Prophet\n",
    "\n",
    "**Time Spans (4):**\n",
    "1. Daily (1 day)\n",
    "2. Weekly (5 days)\n",
    "3. Monthly (21 days)\n",
    "4. Bi-Monthly (42 days)\n",
    "\n",
    "**Total Combinations:** 16 models (4 models × 4 time spans)\n",
    "\n",
    "**Stocks:** 10 oldest stocks from S&P 500 dataset\n",
    "\n",
    "**Technique:** Expanding window with technical features\n",
    "\n",
    "**Goal:** Analyze how time span affects model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f037b0",
   "metadata": {},
   "source": [
    "## Phase 1: Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce308bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, MultiHeadAttention, LayerNormalization, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Time Series\n",
    "from prophet import Prophet\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2289721",
   "metadata": {},
   "source": [
    "### 1.1 Load 10 Oldest Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31105af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ 10 Oldest Stocks Selected:\n",
      "Ticker Start_Date    Years\n",
      "    PG 1962-01-02 60.52293\n",
      "   CNP 1962-01-02 60.52293\n",
      "   CVX 1962-01-02 60.52293\n",
      "   CAT 1962-01-02 60.52293\n",
      "   DIS 1962-01-02 60.52293\n",
      "   DTE 1962-01-02 60.52293\n",
      "    ED 1962-01-02 60.52293\n",
      "    BA 1962-01-02 60.52293\n",
      "    GE 1962-01-02 60.52293\n",
      "   HON 1962-01-02 60.52293\n",
      "✓ Loaded PG: 15236 records from 1962-01-02 to 2022-07-12\n",
      "✓ Loaded CNP: 15236 records from 1962-01-02 to 2022-07-12\n",
      "✓ Loaded CVX: 15236 records from 1962-01-02 to 2022-07-12\n",
      "✓ Loaded CAT: 15236 records from 1962-01-02 to 2022-07-12\n",
      "✓ Loaded DIS: 15236 records from 1962-01-02 to 2022-07-12\n",
      "✓ Loaded DTE: 15236 records from 1962-01-02 to 2022-07-12\n",
      "✓ Loaded ED: 15236 records from 1962-01-02 to 2022-07-12\n",
      "✓ Loaded BA: 15236 records from 1962-01-02 to 2022-07-12\n",
      "✓ Loaded GE: 15236 records from 1962-01-02 to 2022-07-12\n",
      "✓ Loaded HON: 15236 records from 1962-01-02 to 2022-07-12\n",
      "\n",
      "✓ Successfully loaded 10 stocks\n"
     ]
    }
   ],
   "source": [
    "# Load stock data and identify 10 oldest stocks\n",
    "sp500_path = Path('../sp500')\n",
    "csv_files = list(sp500_path.glob('*.csv'))\n",
    "\n",
    "print(f\"Total CSV files found: {len(csv_files)}\")\n",
    "\n",
    "# Load all stocks and find oldest\n",
    "stock_metadata = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    ticker = csv_file.stem\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        stock_start = df['Date'].min()\n",
    "        stock_end = df['Date'].max()\n",
    "        \n",
    "        stock_metadata.append({\n",
    "            'Ticker': ticker,\n",
    "            'Start_Date': stock_start,\n",
    "            'End_Date': stock_end,\n",
    "            'Records': len(df),\n",
    "            'Years': (stock_end - stock_start).days / 365.25\n",
    "        })\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# Create metadata DataFrame\n",
    "metadata_df = pd.DataFrame(stock_metadata).sort_values('Start_Date')\n",
    "\n",
    "# Select 10 oldest stocks\n",
    "oldest_10_tickers = metadata_df.head(10)['Ticker'].tolist()\n",
    "\n",
    "print(f\"\\n✓ 10 Oldest Stocks Selected:\")\n",
    "print(metadata_df.head(10)[['Ticker', 'Start_Date', 'Years']].to_string(index=False))\n",
    "\n",
    "# Load data for these 10 stocks\n",
    "stock_data_dict = {}\n",
    "\n",
    "for ticker in oldest_10_tickers:\n",
    "    csv_file = sp500_path / f\"{ticker}.csv\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    stock_data_dict[ticker] = df\n",
    "    print(f\"✓ Loaded {ticker}: {len(df)} records from {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "\n",
    "print(f\"\\n✓ Successfully loaded {len(stock_data_dict)} stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc2d4c4",
   "metadata": {},
   "source": [
    "### 1.2 Add Technical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a324eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding technical features to all stocks...\n",
      "✓ PG: 15036 rows after feature engineering\n",
      "✓ CNP: 15036 rows after feature engineering\n",
      "✓ CVX: 15036 rows after feature engineering\n",
      "✓ CAT: 15036 rows after feature engineering\n",
      "✓ DIS: 15036 rows after feature engineering\n",
      "✓ DTE: 15036 rows after feature engineering\n",
      "✓ ED: 15036 rows after feature engineering\n",
      "✓ BA: 15036 rows after feature engineering\n",
      "✓ GE: 15036 rows after feature engineering\n",
      "✓ CAT: 15036 rows after feature engineering\n",
      "✓ DIS: 15036 rows after feature engineering\n",
      "✓ DTE: 15036 rows after feature engineering\n",
      "✓ ED: 15036 rows after feature engineering\n",
      "✓ BA: 15036 rows after feature engineering\n",
      "✓ GE: 15036 rows after feature engineering\n",
      "✓ HON: 15036 rows after feature engineering\n",
      "\n",
      "✓ Technical features added successfully!\n",
      "✓ HON: 15036 rows after feature engineering\n",
      "\n",
      "✓ Technical features added successfully!\n"
     ]
    }
   ],
   "source": [
    "def add_technical_features(df):\n",
    "    \"\"\"\n",
    "    Add comprehensive technical indicators with NO LOOK-AHEAD BIAS\n",
    "    \n",
    "    CRITICAL: All indicators are SHIFTED by 1 to ensure we only use past data\n",
    "    This prevents future information from leaking into the model\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Daily returns (shifted to use only past data)\n",
    "    df['Returns'] = df['Close'].pct_change().shift(1)\n",
    "    \n",
    "    # Moving averages (shifted to use only past data)\n",
    "    for window in [5, 10, 20, 30, 50, 100, 200]:\n",
    "        df[f'MA_{window}'] = df['Close'].rolling(window=window).mean().shift(1)\n",
    "    \n",
    "    # Exponential moving averages (shifted)\n",
    "    df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean().shift(1)\n",
    "    df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean().shift(1)\n",
    "    \n",
    "    # MACD (based on shifted EMAs)\n",
    "    df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "    df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean().shift(1)\n",
    "    df['MACD_Hist'] = df['MACD'] - df['MACD_Signal']\n",
    "    \n",
    "    # Bollinger Bands (shifted)\n",
    "    df['BB_Middle'] = df['Close'].rolling(window=20).mean().shift(1)\n",
    "    bb_std = df['Close'].rolling(window=20).std().shift(1)\n",
    "    df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
    "    df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
    "    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']\n",
    "    df['BB_Position'] = (df['Close'].shift(1) - df['BB_Lower']) / df['BB_Width']\n",
    "    \n",
    "    # RSI (shifted)\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = (100 - (100 / (1 + rs))).shift(1)\n",
    "    \n",
    "    # Volatility (shifted)\n",
    "    df['Volatility_10'] = df['Returns'].rolling(window=10).std().shift(1)\n",
    "    df['Volatility_30'] = df['Returns'].rolling(window=30).std().shift(1)\n",
    "    df['Volatility_60'] = df['Returns'].rolling(window=60).std().shift(1)\n",
    "    \n",
    "    # Volume features (shifted)\n",
    "    df['Volume_MA_20'] = df['Volume'].rolling(window=20).mean().shift(1)\n",
    "    df['Volume_Ratio'] = (df['Volume'] / df['Volume_MA_20']).shift(1)\n",
    "    \n",
    "    # Momentum (already using past data with shift, but shift again to be safe)\n",
    "    df['Momentum_5'] = (df['Close'] - df['Close'].shift(5)).shift(1)\n",
    "    df['Momentum_10'] = (df['Close'] - df['Close'].shift(10)).shift(1)\n",
    "    df['Momentum_20'] = (df['Close'] - df['Close'].shift(20)).shift(1)\n",
    "    \n",
    "    # Rate of Change (shifted)\n",
    "    df['ROC_5'] = (((df['Close'] - df['Close'].shift(5)) / df['Close'].shift(5)) * 100).shift(1)\n",
    "    df['ROC_10'] = (((df['Close'] - df['Close'].shift(10)) / df['Close'].shift(10)) * 100).shift(1)\n",
    "    \n",
    "    # Stochastic Oscillator (shifted)\n",
    "    low_14 = df['Low'].rolling(window=14).min()\n",
    "    high_14 = df['High'].rolling(window=14).max()\n",
    "    df['Stoch_K'] = (100 * ((df['Close'] - low_14) / (high_14 - low_14))).shift(1)\n",
    "    df['Stoch_D'] = df['Stoch_K'].rolling(window=3).mean().shift(1)\n",
    "    \n",
    "    # Average True Range (ATR) - shifted\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['Close'].shift())\n",
    "    low_close = np.abs(df['Low'] - df['Close'].shift())\n",
    "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "    true_range = np.max(ranges, axis=1)\n",
    "    df['ATR_14'] = pd.Series(true_range).rolling(window=14).mean().shift(1)\n",
    "    \n",
    "    # On-Balance Volume (shifted)\n",
    "    df['OBV'] = ((np.sign(df['Close'].diff()) * df['Volume']).fillna(0).cumsum()).shift(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add features to all stocks\n",
    "print(\"Adding technical features to all stocks...\")\n",
    "for ticker in oldest_10_tickers:\n",
    "    stock_data_dict[ticker] = add_technical_features(stock_data_dict[ticker])\n",
    "    # Drop NaN rows\n",
    "    stock_data_dict[ticker] = stock_data_dict[ticker].dropna().reset_index(drop=True)\n",
    "    print(f\"✓ {ticker}: {len(stock_data_dict[ticker])} rows after feature engineering\")\n",
    "\n",
    "print(\"\\n✓ Technical features added successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee8577c",
   "metadata": {},
   "source": [
    "### 1.3 Resample Data for Different Time Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ac87f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PG - Daily: 15036 samples\n",
      "✓ PG - Weekly: 2918 samples\n",
      "✓ PG - Monthly: 840 samples\n",
      "✓ PG - Bi-Monthly: 320 samples\n",
      "✓ CNP - Daily: 15036 samples\n",
      "✓ CNP - Weekly: 2918 samples\n",
      "✓ CNP - Monthly: 840 samples\n",
      "✓ CNP - Bi-Monthly: 320 samples\n",
      "✓ CVX - Daily: 15036 samples\n",
      "✓ CVX - Weekly: 2918 samples\n",
      "✓ CVX - Monthly: 840 samples\n",
      "✓ PG - Bi-Monthly: 320 samples\n",
      "✓ CNP - Daily: 15036 samples\n",
      "✓ CNP - Weekly: 2918 samples\n",
      "✓ CNP - Monthly: 840 samples\n",
      "✓ CNP - Bi-Monthly: 320 samples\n",
      "✓ CVX - Daily: 15036 samples\n",
      "✓ CVX - Weekly: 2918 samples\n",
      "✓ CVX - Monthly: 840 samples\n",
      "✓ CVX - Bi-Monthly: 320 samples\n",
      "✓ CAT - Daily: 15036 samples\n",
      "✓ CAT - Weekly: 2918 samples\n",
      "✓ CAT - Monthly: 840 samples\n",
      "✓ CAT - Bi-Monthly: 320 samples\n",
      "✓ DIS - Daily: 15036 samples\n",
      "✓ CVX - Bi-Monthly: 320 samples\n",
      "✓ CAT - Daily: 15036 samples\n",
      "✓ CAT - Weekly: 2918 samples\n",
      "✓ CAT - Monthly: 840 samples\n",
      "✓ CAT - Bi-Monthly: 320 samples\n",
      "✓ DIS - Daily: 15036 samples\n",
      "✓ DIS - Weekly: 2918 samples\n",
      "✓ DIS - Monthly: 840 samples\n",
      "✓ DIS - Bi-Monthly: 320 samples\n",
      "✓ DTE - Daily: 15036 samples\n",
      "✓ DTE - Weekly: 2918 samples\n",
      "✓ DTE - Monthly: 840 samples\n",
      "✓ DTE - Bi-Monthly: 320 samples\n",
      "✓ ED - Daily: 15036 samples\n",
      "✓ DIS - Weekly: 2918 samples\n",
      "✓ DIS - Monthly: 840 samples\n",
      "✓ DIS - Bi-Monthly: 320 samples\n",
      "✓ DTE - Daily: 15036 samples\n",
      "✓ DTE - Weekly: 2918 samples\n",
      "✓ DTE - Monthly: 840 samples\n",
      "✓ DTE - Bi-Monthly: 320 samples\n",
      "✓ ED - Daily: 15036 samples\n",
      "✓ ED - Weekly: 2918 samples\n",
      "✓ ED - Monthly: 840 samples\n",
      "✓ ED - Bi-Monthly: 320 samples\n",
      "✓ BA - Daily: 15036 samples\n",
      "✓ BA - Weekly: 2918 samples\n",
      "✓ ED - Weekly: 2918 samples\n",
      "✓ ED - Monthly: 840 samples\n",
      "✓ ED - Bi-Monthly: 320 samples\n",
      "✓ BA - Daily: 15036 samples\n",
      "✓ BA - Weekly: 2918 samples\n",
      "✓ BA - Monthly: 840 samples\n",
      "✓ BA - Bi-Monthly: 320 samples\n",
      "✓ GE - Daily: 15036 samples\n",
      "✓ GE - Weekly: 2918 samples\n",
      "✓ GE - Monthly: 840 samples\n",
      "✓ GE - Bi-Monthly: 320 samples\n",
      "✓ HON - Daily: 15036 samples\n",
      "✓ HON - Weekly: 2918 samples\n",
      "✓ HON - Monthly: 840 samples\n",
      "✓ HON - Bi-Monthly: 320 samples\n",
      "\n",
      "✓ Data resampled for all time spans!\n",
      "✓ BA - Monthly: 840 samples\n",
      "✓ BA - Bi-Monthly: 320 samples\n",
      "✓ GE - Daily: 15036 samples\n",
      "✓ GE - Weekly: 2918 samples\n",
      "✓ GE - Monthly: 840 samples\n",
      "✓ GE - Bi-Monthly: 320 samples\n",
      "✓ HON - Daily: 15036 samples\n",
      "✓ HON - Weekly: 2918 samples\n",
      "✓ HON - Monthly: 840 samples\n",
      "✓ HON - Bi-Monthly: 320 samples\n",
      "\n",
      "✓ Data resampled for all time spans!\n"
     ]
    }
   ],
   "source": [
    "def resample_stock_data(df, freq='W'):\n",
    "    \"\"\"\n",
    "    Resample stock data to different frequencies\n",
    "    freq: 'D' (daily), 'W' (weekly), 'M' (monthly), '2W' (bi-weekly), '21D' (monthly ~21 trading days)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Resample OHLC data\n",
    "    resampled = df.resample(freq).agg({\n",
    "        'Open': 'first',\n",
    "        'High': 'max',\n",
    "        'Low': 'min',\n",
    "        'Close': 'last',\n",
    "        'Volume': 'sum',\n",
    "        'Dividends': 'sum',\n",
    "        'Stock Splits': 'sum'\n",
    "    })\n",
    "    \n",
    "    # Recalculate features on resampled data\n",
    "    resampled.reset_index(inplace=True)\n",
    "    resampled = add_technical_features(resampled)\n",
    "    \n",
    "    return resampled.dropna()\n",
    "\n",
    "# Create data for all time spans\n",
    "time_spans = {\n",
    "    'Daily': 'D',\n",
    "    'Weekly': 'W',\n",
    "    'Monthly': '21D',  # Approximately 21 trading days per month\n",
    "    'Bi-Monthly': '42D'  # Approximately 42 trading days for 2 months\n",
    "}\n",
    "\n",
    "# Store resampled data for each stock and time span\n",
    "resampled_data = {}\n",
    "\n",
    "for ticker in oldest_10_tickers:\n",
    "    resampled_data[ticker] = {}\n",
    "    original_df = stock_data_dict[ticker].copy()\n",
    "    \n",
    "    for span_name, freq in time_spans.items():\n",
    "        if span_name == 'Daily':\n",
    "            # Use original data\n",
    "            resampled_data[ticker][span_name] = original_df\n",
    "        else:\n",
    "            # Resample to specified frequency\n",
    "            resampled_data[ticker][span_name] = resample_stock_data(original_df, freq)\n",
    "        \n",
    "        print(f\"✓ {ticker} - {span_name}: {len(resampled_data[ticker][span_name])} samples\")\n",
    "\n",
    "print(\"\\n✓ Data resampled for all time spans!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14184a3f",
   "metadata": {},
   "source": [
    "## Phase 2: Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3bb946",
   "metadata": {},
   "source": [
    "### 2.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638c7a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data with NO LOOK-AHEAD BIAS\n",
    "    \n",
    "    CRITICAL FIX: Technical indicators are recalculated using ONLY past data\n",
    "    For each test point, we use expanding window to calculate indicators\n",
    "    This prevents future information leakage\n",
    "    \"\"\"\n",
    "    # Feature columns (exclude metadata and target)\n",
    "    # IMPORTANT: Exclude 'Ticker' (string), keep 'Stock_Encoded' (numeric) for stock identification\n",
    "    exclude_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Ticker']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Split by time (80/20)\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    \n",
    "    # For training: use data up to split point (no look-ahead possible)\n",
    "    train_df = df.iloc[:split_idx].copy()\n",
    "    \n",
    "    # For testing: use expanding window approach\n",
    "    # Each test point uses only historical data (train + previous test points)\n",
    "    test_indices = range(split_idx, len(df))\n",
    "    \n",
    "    X_train = train_df[feature_cols].values\n",
    "    y_train = train_df['Close'].values\n",
    "    \n",
    "    # Build test set with expanding window (no future info)\n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    \n",
    "    for i in test_indices:\n",
    "        # Use all data up to this point (expanding window)\n",
    "        expanding_df = df.iloc[:i+1].copy()\n",
    "        \n",
    "        # Get features for this single test point (last row)\n",
    "        test_point = expanding_df.iloc[-1]\n",
    "        X_test_list.append(test_point[feature_cols].values)\n",
    "        y_test_list.append(test_point['Close'])\n",
    "    \n",
    "    X_test = np.array(X_test_list)\n",
    "    y_test = np.array(y_test_list)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_cols\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # MAPE - handle zero values\n",
    "    mask = y_true != 0\n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if mask.sum() > 0 else 999\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "def create_sequences(X, y, time_steps=60):\n",
    "    \"\"\"Create sequences for LSTM/Transformer\"\"\"\n",
    "    if len(X) <= time_steps:\n",
    "        time_steps = max(1, len(X) // 2)\n",
    "    \n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4577ed39",
   "metadata": {},
   "source": [
    "### 2.2 Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a3ca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Linear Regression function defined\n"
     ]
    }
   ],
   "source": [
    "def train_linear_regression(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train Linear Regression model\"\"\"\n",
    "    # Scale features\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    return y_pred, model, scaler_X\n",
    "\n",
    "print(\"✓ Linear Regression function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c56434",
   "metadata": {},
   "source": [
    "### 2.3 Model 2: BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2abe0fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BiLSTM function defined\n"
     ]
    }
   ],
   "source": [
    "def train_bilstm(X_train, y_train, X_test, y_test, time_steps=30):\n",
    "    \"\"\"Train Bidirectional LSTM model\"\"\"\n",
    "    # Scale data\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    \n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)\n",
    "    \n",
    "    if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Build BiLSTM model\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(64, return_sequences=True), input_shape=(time_steps, X_train_seq.shape[2])),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(32, return_sequences=False)),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_scaled = model.predict(X_test_seq, verbose=0).flatten()\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = y_test[time_steps:]\n",
    "    \n",
    "    return y_pred, y_test_actual, model, (scaler_X, scaler_y)\n",
    "\n",
    "print(\"✓ BiLSTM function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655ee6d",
   "metadata": {},
   "source": [
    "### 2.4 Model 3: Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc42ea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Transformer function defined\n"
     ]
    }
   ],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    \"\"\"Transformer encoder block\"\"\"\n",
    "    # Multi-head attention\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "    \n",
    "    # Feed forward\n",
    "    ff = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    ff = Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "def train_transformer(X_train, y_train, X_test, y_test, time_steps=30):\n",
    "    \"\"\"Train Transformer model\"\"\"\n",
    "    # Scale data\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    \n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)\n",
    "    \n",
    "    if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Build Transformer model\n",
    "    input_layer = Input(shape=(time_steps, X_train_seq.shape[2]))\n",
    "    x = transformer_encoder(input_layer, head_size=32, num_heads=4, ff_dim=64, dropout=0.1)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output_layer = Dense(1)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_scaled = model.predict(X_test_seq, verbose=0).flatten()\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = y_test[time_steps:]\n",
    "    \n",
    "    return y_pred, y_test_actual, model, (scaler_X, scaler_y)\n",
    "\n",
    "print(\"✓ Transformer function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afe73a",
   "metadata": {},
   "source": [
    "### 2.5 Model 4: Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab4c0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prophet function defined\n"
     ]
    }
   ],
   "source": [
    "def train_prophet(df, test_size=0.2):\n",
    "    \"\"\"Train Prophet model\"\"\"\n",
    "    # Split data\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    train_df = df.iloc[:split_idx].copy()\n",
    "    test_df = df.iloc[split_idx:].copy()\n",
    "    \n",
    "    # Prepare data for Prophet\n",
    "    prophet_train = pd.DataFrame({\n",
    "        'ds': train_df['Date'],\n",
    "        'y': train_df['Close']\n",
    "    })\n",
    "    \n",
    "    # Train model\n",
    "    model = Prophet(\n",
    "        daily_seasonality=False,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        changepoint_prior_scale=0.05\n",
    "    )\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore')\n",
    "        model.fit(prophet_train)\n",
    "    \n",
    "    # Predict\n",
    "    future = pd.DataFrame({'ds': test_df['Date']})\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    y_pred = forecast['yhat'].values\n",
    "    y_test = test_df['Close'].values\n",
    "    \n",
    "    return y_pred, y_test, model\n",
    "\n",
    "print(\"✓ Prophet function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcaeb4e",
   "metadata": {},
   "source": [
    "## Phase 3: Train All Models Across All Time Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abbe2479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TRAINING MODELS ACROSS ALL TIME SPANS\n",
      "====================================================================================================\n",
      "Total models to train: 4 time spans × 4 models = 16 models\n",
      "Each model trained on ALL 10 stocks combined\n",
      "\n",
      "Stock Encodings: {'PG': 0, 'CNP': 1, 'CVX': 2, 'CAT': 3, 'DIS': 4, 'DTE': 5, 'ED': 6, 'BA': 7, 'GE': 8, 'HON': 9}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "TIME SPAN: Daily\n",
      "====================================================================================================\n",
      "  Combined dataset: 150360 samples from 10 stocks\n",
      "\n",
      "  Training: Linear Regression\n",
      "  ────────────────────────────────────────────────────────────────────────────────\n",
      "    ✓ RMSE: 1.36\n",
      "    ✓ R²: 0.9995\n",
      "    ✓ MAE: 0.60\n",
      "    ✓ MAPE: 2.38%\n",
      "    ✓ Test Samples: 30072\n",
      "\n",
      "  Training: BiLSTM\n",
      "  ────────────────────────────────────────────────────────────────────────────────\n",
      "    ✓ RMSE: 1.36\n",
      "    ✓ R²: 0.9995\n",
      "    ✓ MAE: 0.60\n",
      "    ✓ MAPE: 2.38%\n",
      "    ✓ Test Samples: 30072\n",
      "\n",
      "  Training: BiLSTM\n",
      "  ────────────────────────────────────────────────────────────────────────────────\n",
      "    ✓ RMSE: 6.63\n",
      "    ✓ R²: 0.9890\n",
      "    ✓ MAE: 3.76\n",
      "    ✓ MAPE: 17.19%\n",
      "    ✓ Test Samples: 30042\n",
      "\n",
      "  Training: Transformer\n",
      "  ────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     77\u001b[0m     time_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInsufficient data for Transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 58\u001b[0m, in \u001b[0;36mtrain_transformer\u001b[1;34m(X_train, y_train, X_test, y_test, time_steps)\u001b[0m\n\u001b[0;32m     48\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     49\u001b[0m     X_train_seq, y_train_seq,\n\u001b[0;32m     50\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m y_pred_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     59\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_scaled\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     60\u001b[0m y_test_actual \u001b[38;5;241m=\u001b[39m y_test[time_steps:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:560\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    558\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m    559\u001b[0m data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n\u001b[1;32m--> 560\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n\u001b[0;32m    562\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(step, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_outputs})\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Storage for all results\n",
    "all_results = []\n",
    "\n",
    "# Train all combinations\n",
    "model_functions = {\n",
    "    'Linear Regression': train_linear_regression,\n",
    "    'BiLSTM': train_bilstm,\n",
    "    'Transformer': train_transformer,\n",
    "    'Prophet': train_prophet\n",
    "}\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TRAINING MODELS ACROSS ALL TIME SPANS\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Total models to train: {len(time_spans)} time spans × {len(model_functions)} models = {len(time_spans) * len(model_functions)} models\")\n",
    "print(f\"Each model trained on ALL {len(oldest_10_tickers)} stocks combined\\n\")\n",
    "\n",
    "# Create stock ticker encoding mapping\n",
    "ticker_mapping = {ticker: idx for idx, ticker in enumerate(oldest_10_tickers)}\n",
    "print(f\"Stock Encodings: {ticker_mapping}\\n\")\n",
    "\n",
    "for span_name in time_spans.keys():\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"TIME SPAN: {span_name}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Combine all stocks for this time span\n",
    "    combined_data = []\n",
    "    for ticker in oldest_10_tickers:\n",
    "        df = resampled_data[ticker][span_name].copy()\n",
    "        df['Stock_Encoded'] = ticker_mapping[ticker]\n",
    "        df['Ticker'] = ticker\n",
    "        combined_data.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "    print(f\"  Combined dataset: {len(combined_df)} samples from {len(oldest_10_tickers)} stocks\")\n",
    "    \n",
    "    # Train each model on combined data\n",
    "    for model_name in model_functions.keys():\n",
    "        print(f\"\\n  Training: {model_name}\")\n",
    "        print(f\"  {'─'*80}\")\n",
    "        \n",
    "        try:\n",
    "            if model_name == 'Prophet':\n",
    "                # Prophet doesn't use stock encoding - train on aggregated close prices\n",
    "                prophet_df = combined_df.groupby('Date').agg({\n",
    "                    'Close': 'mean'  # Average close across all stocks\n",
    "                }).reset_index()\n",
    "                \n",
    "                y_pred, y_test, model = train_prophet(prophet_df)\n",
    "                metrics = evaluate_model(y_test, y_pred, f\"{span_name}_{model_name}\")\n",
    "                test_samples = len(y_test)\n",
    "                \n",
    "            else:\n",
    "                # For ML/DL models, prepare data with stock encoding\n",
    "                X_train, X_test, y_train, y_test, feature_cols = prepare_data(combined_df)\n",
    "                \n",
    "                # Ensure Stock_Encoded is in features\n",
    "                if 'Stock_Encoded' not in feature_cols:\n",
    "                    print(f\"    Warning: Stock_Encoded not in features. Available: {feature_cols[:5]}...\")\n",
    "                \n",
    "                if model_name == 'Linear Regression':\n",
    "                    y_pred, model, scaler = train_linear_regression(X_train, y_train, X_test, y_test)\n",
    "                    metrics = evaluate_model(y_test, y_pred, f\"{span_name}_{model_name}\")\n",
    "                    test_samples = len(y_test)\n",
    "                    \n",
    "                elif model_name == 'BiLSTM':\n",
    "                    time_steps = min(30, len(X_train) // 4)\n",
    "                    result = train_bilstm(X_train, y_train, X_test, y_test, time_steps)\n",
    "                    if result[0] is None:\n",
    "                        raise ValueError(\"Insufficient data for BiLSTM\")\n",
    "                    y_pred, y_test_actual, model, scalers = result\n",
    "                    metrics = evaluate_model(y_test_actual, y_pred, f\"{span_name}_{model_name}\")\n",
    "                    test_samples = len(y_test_actual)\n",
    "                    \n",
    "                elif model_name == 'Transformer':\n",
    "                    time_steps = min(30, len(X_train) // 4)\n",
    "                    result = train_transformer(X_train, y_train, X_test, y_test, time_steps)\n",
    "                    if result[0] is None:\n",
    "                        raise ValueError(\"Insufficient data for Transformer\")\n",
    "                    y_pred, y_test_actual, model, scalers = result\n",
    "                    metrics = evaluate_model(y_test_actual, y_pred, f\"{span_name}_{model_name}\")\n",
    "                    test_samples = len(y_test_actual)\n",
    "            \n",
    "            # Add metadata\n",
    "            metrics['Time_Span'] = span_name\n",
    "            metrics['Model_Type'] = model_name\n",
    "            metrics['Total_Samples'] = len(combined_df)\n",
    "            metrics['Test_Samples'] = test_samples\n",
    "            metrics['Num_Stocks'] = len(oldest_10_tickers)\n",
    "            \n",
    "            all_results.append(metrics)\n",
    "            \n",
    "            print(f\"    ✓ RMSE: {metrics['RMSE']:.2f}\")\n",
    "            print(f\"    ✓ R²: {metrics['R2']:.4f}\")\n",
    "            print(f\"    ✓ MAE: {metrics['MAE']:.2f}\")\n",
    "            print(f\"    ✓ MAPE: {metrics['MAPE']:.2f}%\")\n",
    "            print(f\"    ✓ Test Samples: {test_samples}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ FAILED: {str(e)}\")\n",
    "            import traceback\n",
    "            print(f\"    ✗ Traceback: {traceback.format_exc()[:200]}\")\n",
    "            # Add failed result\n",
    "            all_results.append({\n",
    "                'Model': f\"{span_name}_{model_name}\",\n",
    "                'Time_Span': span_name,\n",
    "                'Model_Type': model_name,\n",
    "                'Total_Samples': len(combined_df),\n",
    "                'Test_Samples': 0,\n",
    "                'Num_Stocks': len(oldest_10_tickers),\n",
    "                'RMSE': 999999,\n",
    "                'MAE': 999999,\n",
    "                'R2': -999,\n",
    "                'MAPE': 999\n",
    "            })\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"✓ TRAINING COMPLETE!\")\n",
    "print(f\"{'='*100}\")\n",
    "print(f\"Total models trained: {len(all_results)}\")\n",
    "print(f\"Expected: {len(time_spans) * len(model_functions)} models (4 time spans × 4 models)\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(f\"\\nResults shape: {results_df.shape}\")\n",
    "print(f\"\\nAll results:\")\n",
    "print(results_df[['Model_Type', 'Time_Span', 'RMSE', 'R2', 'MAPE', 'Total_Samples']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa308d",
   "metadata": {},
   "source": [
    "## Phase 4: Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413d7f3",
   "metadata": {},
   "source": [
    "### 4.1 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbed7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out failed models\n",
    "results_df_clean = results_df[results_df['RMSE'] < 999999].copy()\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"RESULTS SUMMARY - FOCUS ON TIME SPAN COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nTotal models trained: {len(results_df_clean)} successful out of {len(results_df)}\")\n",
    "print(f\"Each model trained on all {results_df_clean['Num_Stocks'].iloc[0]} stocks combined\\n\")\n",
    "\n",
    "# Performance by time span (PRIMARY FOCUS)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"1. PERFORMANCE BY TIME SPAN (Primary Analysis)\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nThis shows how prediction accuracy varies with time aggregation.\")\n",
    "print(\"Daily data may overfit short-term noise; longer spans capture trends.\\n\")\n",
    "\n",
    "span_summary = results_df_clean.groupby('Time_Span').agg({\n",
    "    'RMSE': ['mean', 'std', 'min', 'max'],\n",
    "    'MAE': ['mean', 'std', 'min', 'max'],\n",
    "    'R2': ['mean', 'std', 'min', 'max'],\n",
    "    'MAPE': ['mean', 'std', 'min', 'max']\n",
    "}).round(4)\n",
    "print(span_summary)\n",
    "\n",
    "# Time span rankings\n",
    "print(\"\\n\" + \"─\" * 100)\n",
    "print(\"TIME SPAN RANKINGS (by average RMSE across all models):\")\n",
    "print(\"─\" * 100)\n",
    "span_rankings = results_df_clean.groupby('Time_Span')['RMSE'].mean().sort_values()\n",
    "for rank, (span, rmse) in enumerate(span_rankings.items(), 1):\n",
    "    avg_r2 = results_df_clean[results_df_clean['Time_Span'] == span]['R2'].mean()\n",
    "    avg_mape = results_df_clean[results_df_clean['Time_Span'] == span]['MAPE'].mean()\n",
    "    print(f\"  {rank}. {span:12s}: RMSE={rmse:8.2f}, R²={avg_r2:.4f}, MAPE={avg_mape:.2f}%\")\n",
    "\n",
    "# Performance by model type (SECONDARY)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"2. PERFORMANCE BY MODEL TYPE (Secondary Analysis)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "model_summary = results_df_clean.groupby('Model_Type').agg({\n",
    "    'RMSE': ['mean', 'std', 'min', 'max'],\n",
    "    'MAE': ['mean', 'std', 'min', 'max'],\n",
    "    'R2': ['mean', 'std', 'min', 'max'],\n",
    "    'MAPE': ['mean', 'std', 'min', 'max']\n",
    "}).round(4)\n",
    "print(model_summary)\n",
    "\n",
    "# Model type rankings\n",
    "print(\"\\n\" + \"─\" * 100)\n",
    "print(\"MODEL TYPE RANKINGS (by average RMSE across all time spans):\")\n",
    "print(\"─\" * 100)\n",
    "model_rankings = results_df_clean.groupby('Model_Type')['RMSE'].mean().sort_values()\n",
    "for rank, (model, rmse) in enumerate(model_rankings.items(), 1):\n",
    "    avg_r2 = results_df_clean[results_df_clean['Model_Type'] == model]['R2'].mean()\n",
    "    avg_mape = results_df_clean[results_df_clean['Model_Type'] == model]['MAPE'].mean()\n",
    "    print(f\"  {rank}. {model:20s}: RMSE={rmse:8.2f}, R²={avg_r2:.4f}, MAPE={avg_mape:.2f}%\")\n",
    "\n",
    "# Best model for EACH time span\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"3. BEST MODEL FOR EACH TIME SPAN\")\n",
    "print(\"=\" * 100)\n",
    "print(\"Identifying which model performs best at each time aggregation level.\\n\")\n",
    "\n",
    "time_span_order = ['Daily', 'Weekly', 'Monthly', 'Bi-Monthly']\n",
    "for span in time_span_order:\n",
    "    span_data = results_df_clean[results_df_clean['Time_Span'] == span]\n",
    "    best = span_data.nsmallest(1, 'RMSE').iloc[0]\n",
    "    print(f\"\\n{span}:\")\n",
    "    print(f\"  Best Model: {best['Model_Type']}\")\n",
    "    print(f\"  RMSE: {best['RMSE']:.2f}\")\n",
    "    print(f\"  R²: {best['R2']:.4f}\")\n",
    "    print(f\"  MAPE: {best['MAPE']:.2f}%\")\n",
    "    print(f\"  Test Samples: {best['Test_Samples']}\")\n",
    "\n",
    "# Detailed comparison table\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"4. COMPLETE RESULTS TABLE\")\n",
    "print(\"=\" * 100)\n",
    "print(results_df_clean[['Model_Type', 'Time_Span', 'RMSE', 'MAE', 'R2', 'MAPE']].sort_values(['Time_Span', 'RMSE']).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358839fd",
   "metadata": {},
   "source": [
    "### 4.2 Time Span vs Accuracy Graphs for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Time Span vs Accuracy plots - PRIMARY ANALYSIS\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_to_plot = ['RMSE', 'MAE', 'R2', 'MAPE']\n",
    "model_types = results_df_clean['Model_Type'].unique()\n",
    "time_span_order = ['Daily', 'Weekly', 'Monthly', 'Bi-Monthly']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for model_type in model_types:\n",
    "        model_data = results_df_clean[results_df_clean['Model_Type'] == model_type]\n",
    "        \n",
    "        # Get metric for each time span\n",
    "        span_metrics = []\n",
    "        for span in time_span_order:\n",
    "            span_data = model_data[model_data['Time_Span'] == span]\n",
    "            if len(span_data) > 0:\n",
    "                span_metrics.append(span_data[metric].values[0])\n",
    "            else:\n",
    "                span_metrics.append(None)\n",
    "        \n",
    "        # Plot with larger markers and thicker lines\n",
    "        ax.plot(time_span_order, span_metrics, marker='o', linewidth=3, markersize=10, label=model_type)\n",
    "    \n",
    "    ax.set_xlabel('Time Span (Aggregation Level)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=13, fontweight='bold')\n",
    "    ax.set_title(f'{metric} vs Time Span\\n(How prediction accuracy changes with time aggregation)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=11)\n",
    "    ax.tick_params(axis='y', labelsize=11)\n",
    "    \n",
    "    # Add annotation for best time span\n",
    "    best_span_idx = span_metrics.index(min(span_metrics)) if metric in ['RMSE', 'MAE', 'MAPE'] else span_metrics.index(max(span_metrics))\n",
    "    ax.axvline(x=best_span_idx, color='red', linestyle=':', alpha=0.3, linewidth=2)\n",
    "\n",
    "plt.suptitle('PRIMARY ANALYSIS: Model Performance Across Different Time Spans\\n' + \n",
    "             '(Each model trained on all 10 stocks combined)', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('time_span_vs_accuracy_all_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Time span vs accuracy plot saved as 'time_span_vs_accuracy_all_metrics.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d24ac5c",
   "metadata": {},
   "source": [
    "### 4.3 Individual Model Performance Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate detailed plots for each model showing time span impact\n",
    "for model_type in model_types:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    model_data = results_df_clean[results_df_clean['Model_Type'] == model_type]\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get metrics for each time span\n",
    "        span_values = []\n",
    "        for span in time_span_order:\n",
    "            span_data = model_data[model_data['Time_Span'] == span]\n",
    "            if len(span_data) > 0:\n",
    "                span_values.append(span_data[metric].values[0])\n",
    "            else:\n",
    "                span_values.append(None)\n",
    "        \n",
    "        # Create bar chart for better comparison\n",
    "        bars = ax.bar(time_span_order, span_values, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        \n",
    "        # Color bars by performance (green=good, red=bad)\n",
    "        if metric in ['RMSE', 'MAE', 'MAPE']:\n",
    "            # Lower is better\n",
    "            colors = plt.cm.RdYlGn_r([(v - min(span_values))/(max(span_values) - min(span_values)) for v in span_values])\n",
    "        else:\n",
    "            # Higher is better (R²)\n",
    "            colors = plt.cm.RdYlGn([(v - min(span_values))/(max(span_values) - min(span_values)) for v in span_values])\n",
    "        \n",
    "        for bar, color in zip(bars, colors):\n",
    "            bar.set_facecolor(color)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (span, val) in enumerate(zip(time_span_order, span_values)):\n",
    "            ax.text(i, val, f'{val:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.set_xlabel('Time Span', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel(metric, fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{metric} by Time Span', fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    fig.suptitle(f'{model_type} - Performance Across Time Spans\\n(Trained on all 10 stocks combined)', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_type.replace(\"/\", \"_\")}_time_span_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ {model_type} analysis saved\")\n",
    "\n",
    "print(\"\\n✓ All individual model charts saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d075fcf",
   "metadata": {},
   "source": [
    "### 4.4 Heatmap Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmaps for Model Type vs Time Span\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Pivot table for heatmap\n",
    "    pivot_data = results_df_clean.pivot_table(\n",
    "        values=metric,\n",
    "        index='Model_Type',\n",
    "        columns='Time_Span',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Reorder columns\n",
    "    pivot_data = pivot_data[time_span_order]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(pivot_data, annot=True, fmt='.2f', cmap='RdYlGn_r' if metric in ['RMSE', 'MAE', 'MAPE'] else 'RdYlGn',\n",
    "                ax=ax, cbar_kws={'label': metric}, linewidths=1, linecolor='black')\n",
    "    \n",
    "    ax.set_title(f'{metric} - Model Type vs Time Span', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Time Span', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Model Type', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_time_span_heatmaps.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Heatmaps saved as 'model_time_span_heatmaps.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816db73",
   "metadata": {},
   "source": [
    "### 4.5 Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"BEST MODEL SELECTION - BY TIME SPAN\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nFocus: Identifying best-performing model at each time aggregation level.\")\n",
    "print(\"Note: Daily models may perform better numerically but focus on MONTHLY for long-term prediction.\\n\")\n",
    "\n",
    "# Best model for each time span\n",
    "print(\"\\n1. Best Model for Each Time Span (by RMSE):\")\n",
    "print(\"=\" * 100)\n",
    "for span in time_span_order:\n",
    "    span_data = results_df_clean[results_df_clean['Time_Span'] == span]\n",
    "    best = span_data.nsmallest(1, 'RMSE').iloc[0]\n",
    "    \n",
    "    print(f\"\\n{span}:\")\n",
    "    print(f\"  {'─'*80}\")\n",
    "    print(f\"  Best Model: {best['Model_Type']}\")\n",
    "    print(f\"  RMSE: {best['RMSE']:.2f}\")\n",
    "    print(f\"  MAE: {best['MAE']:.2f}\")\n",
    "    print(f\"  R²: {best['R2']:.4f}\")\n",
    "    print(f\"  MAPE: {best['MAPE']:.2f}%\")\n",
    "    print(f\"  Test Samples: {best['Test_Samples']}\")\n",
    "    \n",
    "    # Show all models for this time span\n",
    "    print(f\"\\n  All models for {span}:\")\n",
    "    span_sorted = span_data.sort_values('RMSE')\n",
    "    for idx, row in span_sorted.iterrows():\n",
    "        print(f\"    {row['Model_Type']:20s}: RMSE={row['RMSE']:8.2f}, R²={row['R2']:.4f}, MAPE={row['MAPE']:6.2f}%\")\n",
    "\n",
    "# Highlight monthly performance (for long-term prediction)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"2. MONTHLY TIME SPAN FOCUS (Best for Long-Term Prediction)\")\n",
    "print(\"=\" * 100)\n",
    "monthly_data = results_df_clean[results_df_clean['Time_Span'] == 'Monthly'].sort_values('RMSE')\n",
    "print(\"\\nRanked by RMSE:\")\n",
    "for rank, (idx, row) in enumerate(monthly_data.iterrows(), 1):\n",
    "    print(f\"  {rank}. {row['Model_Type']:20s}: RMSE={row['RMSE']:8.2f}, R²={row['R2']:.4f}, MAPE={row['MAPE']:6.2f}%\")\n",
    "\n",
    "best_monthly = monthly_data.iloc[0]\n",
    "print(f\"\\n★ RECOMMENDED MODEL FOR LONG-TERM PREDICTION:\")\n",
    "print(f\"  Model: {best_monthly['Model_Type']}\")\n",
    "print(f\"  Time Span: Monthly\")\n",
    "print(f\"  RMSE: {best_monthly['RMSE']:.2f}\")\n",
    "print(f\"  R²: {best_monthly['R2']:.4f}\")\n",
    "print(f\"  MAPE: {best_monthly['MAPE']:.2f}%\")\n",
    "\n",
    "# Time span impact analysis\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"3. TIME SPAN IMPACT ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nAverage performance improvement from Daily to Monthly:\\n\")\n",
    "\n",
    "for model in model_types:\n",
    "    model_data = results_df_clean[results_df_clean['Model_Type'] == model]\n",
    "    \n",
    "    daily_rmse = model_data[model_data['Time_Span'] == 'Daily']['RMSE'].values\n",
    "    monthly_rmse = model_data[model_data['Time_Span'] == 'Monthly']['RMSE'].values\n",
    "    \n",
    "    if len(daily_rmse) > 0 and len(monthly_rmse) > 0:\n",
    "        improvement = ((daily_rmse[0] - monthly_rmse[0]) / daily_rmse[0]) * 100\n",
    "        direction = \"↓ Improvement\" if improvement > 0 else \"↑ Degradation\"\n",
    "        print(f\"  {model:20s}: {improvement:+6.2f}% {direction}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"✓ ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a630071",
   "metadata": {},
   "source": [
    "### 4.6 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('model_testing_results.csv', index=False)\n",
    "print(\"✓ Results saved to 'model_testing_results.csv'\")\n",
    "\n",
    "# Save clean results\n",
    "results_df_clean.to_csv('model_testing_results_clean.csv', index=False)\n",
    "print(\"✓ Clean results saved to 'model_testing_results_clean.csv'\")\n",
    "\n",
    "# Save summary statistics\n",
    "with pd.ExcelWriter('model_testing_summary.xlsx') as writer:\n",
    "    results_df_clean.to_excel(writer, sheet_name='All Results', index=False)\n",
    "    model_summary.to_excel(writer, sheet_name='Model Summary')\n",
    "    span_summary.to_excel(writer, sheet_name='Time Span Summary')\n",
    "\n",
    "print(\"✓ Summary statistics saved to 'model_testing_summary.xlsx'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ALL FILES SAVED!\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. model_testing_results.csv - All 16 model results\")\n",
    "print(\"  2. model_testing_results_clean.csv - Only successful models\")\n",
    "print(\"  3. model_testing_summary.xlsx - Summary statistics in Excel\")\n",
    "print(\"  4. time_span_vs_accuracy_all_metrics.png - PRIMARY: Time span comparison\")\n",
    "print(\"  5. [Model]_time_span_analysis.png - Individual model analysis (4 files)\")\n",
    "print(\"  6. model_time_span_heatmaps.png - Heatmap visualizations\")\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"✓ Total models trained: {len(results_df_clean)} (4 models × 4 time spans)\")\n",
    "print(f\"✓ Each model trained on ALL {results_df_clean['Num_Stocks'].iloc[0]} stocks combined\")\n",
    "print(f\"✓ Focus on MONTHLY time span for long-term prediction reliability\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c9051",
   "metadata": {},
   "source": [
    "## Phase 5: Volatility Prediction with BiLSTM\n",
    "\n",
    "**NEW APPROACH: Predicting Daily Price Deviations (Volatility)**\n",
    "\n",
    "Instead of predicting absolute prices, we now predict:\n",
    "- **Daily Returns**: `(Close_today - Close_yesterday) / Close_yesterday`\n",
    "- **Focus**: Capturing market volatility and price movements\n",
    "- **Benefit**: More stationary data, better generalization across different price levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f002f",
   "metadata": {},
   "source": [
    "### 5.1 Prepare Volatility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb68857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_volatility_data(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for volatility (daily returns) prediction\n",
    "    Target: Daily price change percentage (deviation from previous day)\n",
    "    \n",
    "    NO LOOK-AHEAD BIAS: Uses only historical data for features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate daily returns (target variable)\n",
    "    df['Daily_Return'] = df['Close'].pct_change() * 100  # In percentage\n",
    "    \n",
    "    # Add lagged returns as features (past volatility)\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        df[f'Return_Lag_{lag}'] = df['Daily_Return'].shift(lag)\n",
    "    \n",
    "    # Add lagged volatility measures\n",
    "    df['Volatility_5'] = df['Daily_Return'].shift(1).rolling(window=5).std()\n",
    "    df['Volatility_10'] = df['Daily_Return'].shift(1).rolling(window=10).std()\n",
    "    df['Volatility_20'] = df['Daily_Return'].shift(1).rolling(window=20).std()\n",
    "    \n",
    "    # Add price-based features (lagged to avoid look-ahead)\n",
    "    df['Price_Change_1'] = df['Close'].pct_change(1).shift(1) * 100\n",
    "    df['Price_Change_5'] = df['Close'].pct_change(5).shift(1) * 100\n",
    "    \n",
    "    # Volume changes (lagged)\n",
    "    df['Volume_Change'] = df['Volume'].pct_change().shift(1) * 100\n",
    "    \n",
    "    # High-Low range (lagged)\n",
    "    df['HL_Range'] = ((df['High'] - df['Low']) / df['Close']).shift(1) * 100\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Feature columns for volatility prediction\n",
    "    exclude_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', \n",
    "                    'Stock Splits', 'Ticker', 'Daily_Return']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Split by time\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    \n",
    "    train_df = df.iloc[:split_idx].copy()\n",
    "    test_df = df.iloc[split_idx:].copy()\n",
    "    \n",
    "    X_train = train_df[feature_cols].values\n",
    "    y_train = train_df['Daily_Return'].values\n",
    "    X_test = test_df[feature_cols].values\n",
    "    y_test = test_df['Daily_Return'].values\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_cols\n",
    "\n",
    "print(\"✓ Volatility data preparation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaa24d",
   "metadata": {},
   "source": [
    "### 5.2 BiLSTM for Volatility Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0714b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bilstm_volatility(X_train, y_train, X_test, y_test, time_steps=20):\n",
    "    \"\"\"\n",
    "    Train BiLSTM model for volatility (daily returns) prediction\n",
    "    \n",
    "    Key differences from price prediction:\n",
    "    - Predicts percentage change, not absolute price\n",
    "    - Uses shorter time steps (volatility is more recent-dependent)\n",
    "    - Different evaluation metrics (directional accuracy, volatility MAE)\n",
    "    \"\"\"\n",
    "    # Scale features\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    \n",
    "    # For volatility, we typically don't scale the target (returns are already normalized)\n",
    "    # But we'll use a light scaling for stability\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)\n",
    "    \n",
    "    if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Build BiLSTM model for volatility\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(128, return_sequences=True), input_shape=(time_steps, X_train_seq.shape[2])),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)  # Predict single return value\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.15,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_scaled = model.predict(X_test_seq, verbose=0).flatten()\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = y_test[time_steps:]\n",
    "    \n",
    "    return y_pred, y_test_actual, model, (scaler_X, scaler_y), history\n",
    "\n",
    "print(\"✓ BiLSTM volatility model function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8695281",
   "metadata": {},
   "source": [
    "### 5.3 Train Volatility Models Across Time Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for volatility results\n",
    "volatility_results = []\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TRAINING BiLSTM FOR VOLATILITY PREDICTION\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Objective: Predict daily price deviations (returns), NOT absolute prices\")\n",
    "print(f\"Training on ALL {len(oldest_10_tickers)} stocks combined\\n\")\n",
    "\n",
    "for span_name in time_spans.keys():\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"TIME SPAN: {span_name}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Combine all stocks for this time span\n",
    "    combined_data_vol = []\n",
    "    for ticker in oldest_10_tickers:\n",
    "        df = resampled_data[ticker][span_name].copy()\n",
    "        df['Stock_Encoded'] = ticker_mapping[ticker]\n",
    "        df['Ticker'] = ticker\n",
    "        combined_data_vol.append(df)\n",
    "    \n",
    "    combined_df_vol = pd.concat(combined_data_vol, ignore_index=True)\n",
    "    print(f\"  Combined dataset: {len(combined_df_vol)} samples from {len(oldest_10_tickers)} stocks\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare volatility data\n",
    "        X_train, X_test, y_train, y_test, feature_cols = prepare_volatility_data(combined_df_vol)\n",
    "        \n",
    "        print(f\"  Training volatility BiLSTM...\")\n",
    "        print(f\"  Features: {len(feature_cols)} columns\")\n",
    "        print(f\"  Training samples: {len(X_train)}, Test samples: {len(y_test)}\")\n",
    "        \n",
    "        # Train BiLSTM for volatility\n",
    "        time_steps = min(20, len(X_train) // 4)\n",
    "        result = train_bilstm_volatility(X_train, y_train, X_test, y_test, time_steps)\n",
    "        \n",
    "        if result[0] is None:\n",
    "            raise ValueError(\"Insufficient data for BiLSTM volatility\")\n",
    "        \n",
    "        y_pred, y_test_actual, model, scalers, history = result\n",
    "        \n",
    "        # Evaluate volatility prediction\n",
    "        mae = mean_absolute_error(y_test_actual, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred))\n",
    "        \n",
    "        # Directional accuracy (did we predict the right direction?)\n",
    "        direction_correct = np.sum((y_test_actual > 0) == (y_pred > 0))\n",
    "        direction_accuracy = (direction_correct / len(y_test_actual)) * 100\n",
    "        \n",
    "        # Correlation between predicted and actual returns\n",
    "        correlation = np.corrcoef(y_test_actual, y_pred)[0, 1]\n",
    "        \n",
    "        # R² score\n",
    "        r2 = r2_score(y_test_actual, y_pred)\n",
    "        \n",
    "        volatility_results.append({\n",
    "            'Time_Span': span_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2,\n",
    "            'Direction_Accuracy': direction_accuracy,\n",
    "            'Correlation': correlation,\n",
    "            'Test_Samples': len(y_test_actual),\n",
    "            'Total_Samples': len(combined_df_vol),\n",
    "            'Time_Steps': time_steps\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n  ✓ VOLATILITY PREDICTION RESULTS:\")\n",
    "        print(f\"    MAE (Return %): {mae:.4f}%\")\n",
    "        print(f\"    RMSE (Return %): {rmse:.4f}%\")\n",
    "        print(f\"    R²: {r2:.4f}\")\n",
    "        print(f\"    Direction Accuracy: {direction_accuracy:.2f}%\")\n",
    "        print(f\"    Correlation: {correlation:.4f}\")\n",
    "        print(f\"    Test Samples: {len(y_test_actual)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ FAILED: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"  ✗ Traceback: {traceback.format_exc()[:300]}\")\n",
    "        \n",
    "        volatility_results.append({\n",
    "            'Time_Span': span_name,\n",
    "            'MAE': 999,\n",
    "            'RMSE': 999,\n",
    "            'R2': -999,\n",
    "            'Direction_Accuracy': 50.0,\n",
    "            'Correlation': 0.0,\n",
    "            'Test_Samples': 0,\n",
    "            'Total_Samples': len(combined_df_vol),\n",
    "            'Time_Steps': 0\n",
    "        })\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"✓ VOLATILITY TRAINING COMPLETE!\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Create volatility results DataFrame\n",
    "volatility_df = pd.DataFrame(volatility_results)\n",
    "print(f\"\\nVolatility Results:\")\n",
    "print(volatility_df[['Time_Span', 'MAE', 'RMSE', 'Direction_Accuracy', 'Correlation']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b51cbe",
   "metadata": {},
   "source": [
    "### 5.4 Volatility Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed72445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter successful volatility predictions\n",
    "volatility_df_clean = volatility_df[volatility_df['MAE'] < 999].copy()\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"VOLATILITY PREDICTION ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nTotal successful models: {len(volatility_df_clean)} out of {len(volatility_df)}\")\n",
    "print(f\"\\nKey Insight: Volatility prediction focuses on DIRECTION and MAGNITUDE of price changes\")\n",
    "print(f\"            NOT absolute prices - better for risk management and trading signals\\n\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PERFORMANCE BY TIME SPAN\")\n",
    "print(\"=\" * 100)\n",
    "for idx, row in volatility_df_clean.iterrows():\n",
    "    print(f\"\\n{row['Time_Span']}:\")\n",
    "    print(f\"  MAE (Return %): {row['MAE']:.4f}%\")\n",
    "    print(f\"  RMSE (Return %): {row['RMSE']:.4f}%\")\n",
    "    print(f\"  Direction Accuracy: {row['Direction_Accuracy']:.2f}%\")\n",
    "    print(f\"  Correlation: {row['Correlation']:.4f}\")\n",
    "    print(f\"  R²: {row['R2']:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "time_span_order = ['Daily', 'Weekly', 'Monthly', 'Bi-Monthly']\n",
    "metrics = ['MAE', 'RMSE', 'Direction_Accuracy', 'Correlation']\n",
    "titles = ['MAE (Lower is Better)', 'RMSE (Lower is Better)', \n",
    "          'Direction Accuracy % (Higher is Better)', 'Correlation (Higher is Better)']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    values = []\n",
    "    for span in time_span_order:\n",
    "        span_data = volatility_df_clean[volatility_df_clean['Time_Span'] == span]\n",
    "        if len(span_data) > 0:\n",
    "            values.append(span_data[metric].values[0])\n",
    "        else:\n",
    "            values.append(0)\n",
    "    \n",
    "    bars = ax.bar(time_span_order, values, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Color bars\n",
    "    if metric in ['Direction_Accuracy', 'Correlation']:\n",
    "        colors = plt.cm.RdYlGn([(v - min(values))/(max(values) - min(values) + 0.001) for v in values])\n",
    "    else:\n",
    "        colors = plt.cm.RdYlGn_r([(v - min(values))/(max(values) - min(values) + 0.001) for v in values])\n",
    "    \n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_facecolor(color)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (span, val) in enumerate(zip(time_span_order, values)):\n",
    "        ax.text(i, val, f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Time Span', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('BiLSTM Volatility Prediction Performance Across Time Spans\\n' + \n",
    "             '(Predicting Daily Returns/Deviations, NOT Absolute Prices)', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('volatility_prediction_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Volatility visualization saved as 'volatility_prediction_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f577da",
   "metadata": {},
   "source": [
    "### 5.5 Price Reconstruction from Volatility Predictions\n",
    "\n",
    "**Demonstrating how predicted returns translate to actual prices**\n",
    "\n",
    "We'll take 1-2 stocks and show:\n",
    "1. Predicted returns vs actual returns\n",
    "2. Reconstructed prices from predicted returns vs actual prices\n",
    "3. Cumulative performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5553c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 2 stocks for visualization\n",
    "demo_stocks = [oldest_10_tickers[0], oldest_10_tickers[4]]  # First and fifth stock\n",
    "demo_span = 'Daily'  # Use daily for better visualization\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"VOLATILITY TO PRICE RECONSTRUCTION DEMONSTRATION\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Demonstrating on stocks: {demo_stocks}\")\n",
    "print(f\"Time span: {demo_span}\\n\")\n",
    "\n",
    "reconstruction_results = {}\n",
    "\n",
    "for ticker in demo_stocks:\n",
    "    print(f\"\\n{'─'*100}\")\n",
    "    print(f\"Processing: {ticker}\")\n",
    "    print(f\"{'─'*100}\")\n",
    "    \n",
    "    try:\n",
    "        # Get data for this stock\n",
    "        df = resampled_data[ticker][demo_span].copy()\n",
    "        df['Stock_Encoded'] = ticker_mapping[ticker]\n",
    "        df['Ticker'] = ticker\n",
    "        \n",
    "        # Prepare volatility data\n",
    "        X_train, X_test, y_train, y_test, feature_cols = prepare_volatility_data(df)\n",
    "        \n",
    "        print(f\"Training samples: {len(X_train)}, Test samples: {len(y_test)}\")\n",
    "        \n",
    "        # Train BiLSTM for this stock\n",
    "        time_steps = min(20, len(X_train) // 4)\n",
    "        result = train_bilstm_volatility(X_train, y_train, X_test, y_test, time_steps)\n",
    "        \n",
    "        if result[0] is None:\n",
    "            print(f\"  ✗ Insufficient data for {ticker}\")\n",
    "            continue\n",
    "        \n",
    "        y_pred, y_test_actual, model, scalers, history = result\n",
    "        \n",
    "        # Get the test portion of original dataframe\n",
    "        df_clean = df.copy()\n",
    "        df_clean['Daily_Return'] = df_clean['Close'].pct_change() * 100\n",
    "        df_clean = df_clean.dropna()\n",
    "        \n",
    "        # Calculate split point\n",
    "        split_idx = int(len(df_clean) * 0.8)\n",
    "        test_df = df_clean.iloc[split_idx:].copy().reset_index(drop=True)\n",
    "        \n",
    "        # Align predictions with test data (account for time_steps offset)\n",
    "        test_df = test_df.iloc[time_steps:].reset_index(drop=True)\n",
    "        \n",
    "        # Add predictions\n",
    "        test_df['Predicted_Return'] = y_pred\n",
    "        test_df['Actual_Return'] = y_test_actual\n",
    "        \n",
    "        # Reconstruct prices from predicted returns\n",
    "        # Starting price is the last price before test period\n",
    "        start_price = df_clean.iloc[split_idx + time_steps - 1]['Close']\n",
    "        \n",
    "        # Method 1: Cumulative returns to reconstruct price\n",
    "        test_df['Predicted_Price'] = start_price\n",
    "        test_df['Reconstructed_Price'] = start_price\n",
    "        \n",
    "        for i in range(len(test_df)):\n",
    "            if i == 0:\n",
    "                test_df.loc[i, 'Reconstructed_Price'] = start_price * (1 + test_df.loc[i, 'Predicted_Return'] / 100)\n",
    "            else:\n",
    "                test_df.loc[i, 'Reconstructed_Price'] = test_df.loc[i-1, 'Reconstructed_Price'] * (1 + test_df.loc[i, 'Predicted_Return'] / 100)\n",
    "        \n",
    "        reconstruction_results[ticker] = test_df\n",
    "        \n",
    "        # Calculate metrics\n",
    "        direction_correct = np.sum((test_df['Actual_Return'] > 0) == (test_df['Predicted_Return'] > 0))\n",
    "        direction_accuracy = (direction_correct / len(test_df)) * 100\n",
    "        \n",
    "        price_rmse = np.sqrt(mean_squared_error(test_df['Close'], test_df['Reconstructed_Price']))\n",
    "        price_mape = np.mean(np.abs((test_df['Close'] - test_df['Reconstructed_Price']) / test_df['Close'])) * 100\n",
    "        \n",
    "        return_mae = mean_absolute_error(test_df['Actual_Return'], test_df['Predicted_Return'])\n",
    "        return_corr = np.corrcoef(test_df['Actual_Return'], test_df['Predicted_Return'])[0, 1]\n",
    "        \n",
    "        print(f\"\\n  ✓ Results for {ticker}:\")\n",
    "        print(f\"    Return MAE: {return_mae:.4f}%\")\n",
    "        print(f\"    Direction Accuracy: {direction_accuracy:.2f}%\")\n",
    "        print(f\"    Return Correlation: {return_corr:.4f}\")\n",
    "        print(f\"    Reconstructed Price RMSE: ${price_rmse:.2f}\")\n",
    "        print(f\"    Reconstructed Price MAPE: {price_mape:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed for {ticker}: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"  {traceback.format_exc()[:300]}\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"✓ Reconstruction complete for {len(reconstruction_results)} stocks\")\n",
    "print(f\"{'='*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358bf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price reconstruction for each stock\n",
    "for ticker, test_df in reconstruction_results.items():\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(16, 14))\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted Returns\n",
    "    ax1 = axes[0]\n",
    "    x_axis = range(len(test_df))\n",
    "    \n",
    "    ax1.plot(x_axis, test_df['Actual_Return'], label='Actual Returns', \n",
    "             color='blue', alpha=0.7, linewidth=1.5)\n",
    "    ax1.plot(x_axis, test_df['Predicted_Return'], label='Predicted Returns', \n",
    "             color='red', alpha=0.7, linewidth=1.5)\n",
    "    ax1.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax1.fill_between(x_axis, 0, test_df['Actual_Return'], \n",
    "                     where=(test_df['Actual_Return'] > 0), alpha=0.2, color='green', label='Positive Actual')\n",
    "    ax1.fill_between(x_axis, 0, test_df['Actual_Return'], \n",
    "                     where=(test_df['Actual_Return'] < 0), alpha=0.2, color='red', label='Negative Actual')\n",
    "    \n",
    "    ax1.set_xlabel('Trading Days (Test Period)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Daily Return (%)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title(f'{ticker} - Actual vs Predicted Daily Returns', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='best', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Actual vs Reconstructed Prices\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    ax2.plot(x_axis, test_df['Close'], label='Actual Price', \n",
    "             color='blue', linewidth=2.5, alpha=0.8)\n",
    "    ax2.plot(x_axis, test_df['Reconstructed_Price'], label='Reconstructed Price (from predicted returns)', \n",
    "             color='red', linewidth=2, alpha=0.8, linestyle='--')\n",
    "    \n",
    "    # Fill area between actual and predicted\n",
    "    ax2.fill_between(x_axis, test_df['Close'], test_df['Reconstructed_Price'], \n",
    "                     alpha=0.2, color='orange', label='Prediction Error')\n",
    "    \n",
    "    ax2.set_xlabel('Trading Days (Test Period)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Stock Price ($)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title(f'{ticker} - Actual vs Reconstructed Prices', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='best', fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Cumulative Returns Comparison\n",
    "    ax3 = axes[2]\n",
    "    \n",
    "    # Calculate cumulative returns\n",
    "    actual_cumulative = (1 + test_df['Actual_Return'] / 100).cumprod() - 1\n",
    "    predicted_cumulative = (1 + test_df['Predicted_Return'] / 100).cumprod() - 1\n",
    "    \n",
    "    ax3.plot(x_axis, actual_cumulative * 100, label='Actual Cumulative Return', \n",
    "             color='blue', linewidth=2.5, alpha=0.8)\n",
    "    ax3.plot(x_axis, predicted_cumulative * 100, label='Predicted Cumulative Return', \n",
    "             color='red', linewidth=2, alpha=0.8, linestyle='--')\n",
    "    ax3.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Fill positive/negative areas\n",
    "    ax3.fill_between(x_axis, 0, actual_cumulative * 100, \n",
    "                     where=(actual_cumulative > 0), alpha=0.2, color='green')\n",
    "    ax3.fill_between(x_axis, 0, actual_cumulative * 100, \n",
    "                     where=(actual_cumulative < 0), alpha=0.2, color='red')\n",
    "    \n",
    "    ax3.set_xlabel('Trading Days (Test Period)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Cumulative Return (%)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title(f'{ticker} - Cumulative Returns: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "    ax3.legend(loc='best', fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{ticker} - Volatility Prediction to Price Reconstruction\\n' + \n",
    "                 'Demonstrating how predicted returns translate to actual prices', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{ticker}_volatility_reconstruction.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Saved visualization: {ticker}_volatility_reconstruction.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"KEY INSIGHTS FROM PRICE RECONSTRUCTION\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\n1. Return Prediction Quality:\")\n",
    "print(\"   - Blue/Red overlap in Plot 1 shows return prediction accuracy\")\n",
    "print(\"   - Direction matters more than exact magnitude for trading\")\n",
    "print(\"\\n2. Price Reconstruction:\")\n",
    "print(\"   - Plot 2 shows cumulative effect of return predictions\")\n",
    "print(\"   - Small return errors compound over time\")\n",
    "print(\"   - Orange shaded area = cumulative prediction error\")\n",
    "print(\"\\n3. Cumulative Performance:\")\n",
    "print(\"   - Plot 3 shows portfolio-level performance\")\n",
    "print(\"   - If curves track together, volatility model captures trends\")\n",
    "print(\"   - Useful for risk management and position sizing\")\n",
    "print(\"\\n✓ Volatility models predict DIRECTION and RISK, not exact prices\")\n",
    "print(\"✓ Better suited for trading signals than absolute price forecasting\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958a950",
   "metadata": {},
   "source": [
    "### 5.6 Final Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save volatility results\n",
    "volatility_df.to_csv('volatility_prediction_results.csv', index=False)\n",
    "print(\"✓ Volatility results saved to 'volatility_prediction_results.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FINAL SUMMARY - TWO-PHASE MODEL TESTING COMPLETE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n\" + \"─\" * 100)\n",
    "print(\"PHASE 1: PRICE PREDICTION (4 Models × 4 Time Spans = 16 Models)\")\n",
    "print(\"─\" * 100)\n",
    "print(f\"✓ Models tested: Linear Regression, BiLSTM, Transformer, Prophet\")\n",
    "print(f\"✓ Time spans: Daily, Weekly, Monthly, Bi-Monthly\")\n",
    "print(f\"✓ Training approach: All 10 stocks combined with stock encoding\")\n",
    "print(f\"✓ Data leak prevention: NO LOOK-AHEAD BIAS (expanding window)\")\n",
    "print(f\"✓ Successful models: {len(results_df_clean)} out of {len(results_df)}\")\n",
    "\n",
    "if len(results_df_clean) > 0:\n",
    "    best_price = results_df_clean.nsmallest(1, 'RMSE').iloc[0]\n",
    "    print(f\"\\n  Best Price Prediction Model:\")\n",
    "    print(f\"  • {best_price['Model_Type']} on {best_price['Time_Span']} time span\")\n",
    "    print(f\"  • RMSE: {best_price['RMSE']:.2f}\")\n",
    "    print(f\"  • R²: {best_price['R2']:.4f}\")\n",
    "    print(f\"  • MAPE: {best_price['MAPE']:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"─\" * 100)\n",
    "print(\"PHASE 2: VOLATILITY PREDICTION (BiLSTM × 4 Time Spans = 4 Models)\")\n",
    "print(\"─\" * 100)\n",
    "print(f\"✓ Objective: Predict daily returns/deviations, NOT absolute prices\")\n",
    "print(f\"✓ Model: BiLSTM optimized for volatility prediction\")\n",
    "print(f\"✓ Key metric: Direction Accuracy (predicting up/down movement)\")\n",
    "print(f\"✓ Data leak prevention: All features are LAGGED (no future information)\")\n",
    "print(f\"✓ Successful models: {len(volatility_df_clean)} out of {len(volatility_df)}\")\n",
    "\n",
    "if len(volatility_df_clean) > 0:\n",
    "    best_vol = volatility_df_clean.nsmallest(1, 'MAE').iloc[0]\n",
    "    best_dir = volatility_df_clean.nlargest(1, 'Direction_Accuracy').iloc[0]\n",
    "    \n",
    "    print(f\"\\n  Best Volatility Prediction (Lowest MAE):\")\n",
    "    print(f\"  • {best_vol['Time_Span']} time span\")\n",
    "    print(f\"  • MAE: {best_vol['MAE']:.4f}%\")\n",
    "    print(f\"  • Direction Accuracy: {best_vol['Direction_Accuracy']:.2f}%\")\n",
    "    print(f\"  • Correlation: {best_vol['Correlation']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Best Direction Prediction (Highest Accuracy):\")\n",
    "    print(f\"  • {best_dir['Time_Span']} time span\")\n",
    "    print(f\"  • Direction Accuracy: {best_dir['Direction_Accuracy']:.2f}%\")\n",
    "    print(f\"  • MAE: {best_dir['MAE']:.4f}%\")\n",
    "    print(f\"  • Correlation: {best_dir['Correlation']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"KEY IMPROVEMENTS IMPLEMENTED\")\n",
    "print(\"=\" * 100)\n",
    "print(\"1. ✓ FIXED LOOK-AHEAD BIAS:\")\n",
    "print(\"   - Expanding window approach for test data\")\n",
    "print(\"   - All technical indicators use only historical data\")\n",
    "print(\"   - No future information leakage in train/test split\")\n",
    "print(\"\\n2. ✓ ADDED VOLATILITY PREDICTION:\")\n",
    "print(\"   - BiLSTM trained to predict returns, not prices\")\n",
    "print(\"   - Direction accuracy as key metric\")\n",
    "print(\"   - Better for risk management and trading signals\")\n",
    "print(\"\\n3. ✓ COMPREHENSIVE EVALUATION:\")\n",
    "print(\"   - Price prediction: RMSE, R², MAPE\")\n",
    "print(\"   - Volatility prediction: MAE, Direction Accuracy, Correlation\")\n",
    "print(\"   - Time span comparison for both approaches\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ALL FILES SAVED\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nPhase 1 - Price Prediction:\")\n",
    "print(\"  • model_testing_results.csv\")\n",
    "print(\"  • model_testing_results_clean.csv\")\n",
    "print(\"  • model_testing_summary.xlsx\")\n",
    "print(\"  • time_span_vs_accuracy_all_metrics.png\")\n",
    "print(\"  • [Model]_time_span_analysis.png (4 files)\")\n",
    "print(\"  • model_time_span_heatmaps.png\")\n",
    "print(\"\\nPhase 2 - Volatility Prediction:\")\n",
    "print(\"  • volatility_prediction_results.csv\")\n",
    "print(\"  • volatility_prediction_results.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"✓✓✓ COMPLETE MODEL TESTING PIPELINE FINISHED ✓✓✓\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "print(\"─\" * 100)\n",
    "print(\"• For PRICE forecasting: Use Monthly time span (best long-term reliability)\")\n",
    "print(\"• For VOLATILITY/RISK: Use BiLSTM volatility model (direction prediction)\")\n",
    "print(\"• For TRADING signals: Combine both - volatility for entry/exit timing\")\n",
    "print(\"• Data quality: NO LOOK-AHEAD BIAS ensures realistic performance estimates\")\n",
    "print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
